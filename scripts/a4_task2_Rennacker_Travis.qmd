---
title: "Word Frequency and Lexicon Analysis in Hitchhiker's"
author: "Travis Renacker"
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    toc-location: left
    toc-depth: 2
    number_sections: true
    code-fold: true
    code-tools: true
    code-summary: "Show Code"
    embed-resources: true
    theme: sandstone
execute:
  eval: true
  message: false
  warning: false

---

# Data:
**Adams, D. (1979). The Hitchhiker's Guide to the Galaxy. Pan Books. Retrieved March 13, 2025, from https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf**

# Overview 

What follows is a word choice analysis of the Hitchhiker's Guide to the Galaxy. The book will be stripped down and frequency of words words choice as well as number of times words will used are shown on two separate bar charts. The bing lexicon will be used to asses positivity or negativity according to the bing sentiment analysis. This will be shown on a bar chart. This was done for academic purposes only and should not  be used as a template of analysis beyond the rudimentary observation of word choice for most frequent words and the authors ability to use over three thousand words, not including stop words, only once. 

### Psuedo Code: 

- Import pdf with pdftools

- EDA of text through page review

- Seperate words, delete spacing, remove punctuation etc 

- Sum up word usage for total times each word was used

- Create plots for word frequency and word choice

- bing lexicon sentimental analysis to find positive and negative words


#### Libraries

```{r setup}

library(tidyverse)
library(tidytext)
library(pdftools)
library(ggwordcloud)
library(textdata)
```


#### Text 
```{r}
hhgttg_text <- pdftools::pdf_text(here::here('data', 'Douglas_Adams_The_Hitchhikers_Guide_to_the_Galaxy_1995.pdf'))

```


#### EDA
```{r}
hitch_3 <- hhgttg_text[3]
hitch_2 <- hhgttg_text[2]
hitch_1 <- hhgttg_text[1]
```


#### Create Tidy data
```{r}
#| output: false


# Assuming you have your PDF text in a variable called text_content
hitch_words <- tibble(text = hhgttg_text) %>%
  # First, normalize whitespace
  mutate(text = str_replace_all(text, "\\s+", " ")) %>%
  
  # Replace apostrophes with a unique placeholder BEFORE any tokenization
  mutate(text = str_replace_all(text, "'", "TEMP_APOSTROPHE")) %>%
  
  
  
  # Add spaces around other punctuation to prepare for tokenization
  mutate(text = str_replace_all(text, "([[:punct:]])(?!')", " \\1 ")) %>%
  
  # Split the text into individual words
  separate_rows(text, sep = " ") %>%
  
  anti_join(stop_words, by = c("text" = "word")) %>%
  
  # Remove empty strings
  filter(text != "") %>%
  
  # Remove tokens that are just punctuation
  filter(!str_detect(text, "^[[:punct:]]+$")) %>%
  
  # Remove remaining punctuation EXCEPT our apostrophe placeholder
  mutate(text = str_replace_all(text, "[\"(),;:.!?\\[\\]{}â€“-]", "")) %>%
  
  # Restore real apostrophes
  mutate(text = str_replace_all(text, "TEMP_APOSTROPHE", "'")) %>%
  
  # Convert to lowercase
  mutate(text = tolower(text)) %>%
  
  # Add word index
  mutate(word_index = row_number()) %>%
  
  # Rearrange columns
  select(word_index, text)

# View the result
head(hitch_words, 20)
```

#### Word Count 

```{r}
hitch_wordcount <- hitch_words  |>
  count(text)
```

#### Group and Sum

```{r}
#| output: false

hitch_clean <- hitch_wordcount |> 
  anti_join(stop_words, by = c("text" = "word")) 

total_wordcount <- hitch_clean |>
  group_by(text) |>
  summarize(count= sum(n),
            .groups = "drop")



head(total_wordcount)


```


#### Capitalize names remove conjunctions and abrivitations
 
```{r}

# List of names to capitalize because they are names believe it or not
names_to_capitalize <- c("dent", "prosser", "arthur", "ford", "prefect", "trillian", "zaphod", "beeblebrox", "marvin", "slartibartfast", "agrajag", "damogran")


#filter for the 20 most common used words and make a bar chart showing the most common words 
most_common <- total_wordcount |> 
  filter(!str_detect(text, "\\bdidn\\b") & 
         !str_detect(text, "\\bll\\b") & 
         !str_detect(text, "\\bdon\\b") & 
         !str_detect(text, "\\ber\\b") & 
           !str_detect(text, "\\bhey\\b") &
         !str_detect(text, "\\bve\\b")) |>  # Remove contraction stop words from the top 30 
   mutate(text = ifelse(text %in% names_to_capitalize, str_to_title(text), text)) |>
  filter(count > 40) |>           # create top 30 list
  arrange(desc(count))            # largest first

#cross reference the list without our word work to see if it wordked

most_common1 <- total_wordcount |> 

  filter(count > 30) |>           # Keep rows where 'count' is greater than 30
  arrange(desc(count))            # Sort by descending 'count'
```

#### Lexicon
```{r}
bing_lex <- get_sentiments(lexicon = "bing")
```

```{r}
hitch_lex <- total_wordcount |> mutate(word=text)
```

```{r}
hitch_bing <- hitch_lex |> 
  inner_join(bing_lex, by = 'word') ### why inner_join?
```

#### Filter more stop words
```{r}

diverse_word_usage <- total_wordcount |>
  
filter(!str_detect(text, "\\bdidn\\b") & 
         !str_detect(text, "\\bll\\b") & 
         !str_detect(text, "\\bdon\\b") & 
         !str_detect(text, "\\ber\\b") & 
          !str_detect(text, "\\bhey\\b") &
         !str_detect(text, "\\bve\\b")) |>  # Remove contraction stop words from the top 30 
  
    mutate(text = ifelse(text %in% names_to_capitalize, str_to_title(text), text)) |> #capitalize names like before
  arrange(desc(count))



```


#### Word Fequency

```{r}

# Create a data frame showing how many words appear with specific frequencies
word_frequency_distribution <- diverse_word_usage %>%
  # Group by count (number of times a word appears)
  group_by(count) %>%
  # Count how many words have this frequency
  summarize(number_of_words = n()) %>%
  # Sort from highest to lowest frequency
  arrange(desc(count)) |> 
  filter(number_of_words > 2) #filter for greater than two words

``` 


# Plots

::: panel-tabset

## Frequently Used

```{r}
#| fig-cap: "**Fig 1.** Most Frequent. Three of the main characters names appear at the top of the list, followed by 'planet' and 'computer.'"

ggplot(most_common, aes(y = reorder(text, count), x = count)) +
  geom_bar(stat = "identity", 
           fill = "cadetblue1", 
           width = 1,                # Width = 1 removes gaps between bars
           color = "black",          # Adds black border
           linewidth = 0.5) +             # Border thickness
 labs(y = "Top 24", x = NULL, title = "Most Common Words in the 'The Hitch Hiker's Guide to the Galaxy'") +
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 11),  # Increased text size
    axis.title = element_text(size = 12, face = "bold"),
    plot.title = element_text(size = 14, face = "bold"),
    plot.margin = margin(b = 20, l = 20, r = 20, t = 20),
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank()
  ) +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05)))

```


## Word Choice 

```{r}
#| fig-cap: "**Fig 2.** Word Choice. The author chose to use the vast majority of words only once or twice. "


# Convert count to character for y-axis labels
word_frequency_distribution$count_char <- as.character(word_frequency_distribution$count) 

ggplot(word_frequency_distribution, aes(y = reorder(count_char, count), x = number_of_words)) +
  geom_bar(stat = "identity", 
           fill = "cadetblue1", 
           width = 1.5,             # Increased width for thicker bars
           color = "black",         # Adds black border
           linewidth = 0.5) +       # Border thickness
  labs(y = "Word Frequency (occurrences)", 
       x = "Number of Different Words") +
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 12),  # Increased text size for y-axis labels
    axis.title = element_text(size = 12, face = "bold"),
    plot.margin = margin(b = 20, l = 30, r = 20, t = 20),  # Increased left margin
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    axis.ticks.y = element_line(color = "gray70"),  # Add light ticks for better readability
    panel.spacing.y = unit(1, "cm")  # Add spacing between y-axis elements
  ) +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(clip = "off")  # Prevents clipping of labels
```


## Lexicon

```{r}
#| fig-cap: "**Fig 3.** Vibe. Positive and Negative words assessed through bing lexicon sentiment analysis. Shown below is the number of negative and postive words according to bing sentimental lexicon. "

# find log ratio score overall:
bing_log_ratio_book <- hitch_bing |> 
  summarize(n_pos = sum(sentiment == 'positive'),
            n_neg = sum(sentiment == 'negative'))

bing_log_ratio_long <- bing_log_ratio_book |>
  pivot_longer(cols = c(n_pos, n_neg), 
               names_to = "type", 
               values_to = "count")

ggplot(data = bing_log_ratio_long, 
       aes(x = type, y = count, fill = type)) +  # Changed color to fill for bar charts
  geom_col(color = "black", width = 0.7) +  # Added border and adjusted width
  labs(
    title = "Lexicon Analysis",
    x = NULL,  # Remove x-axis label as the column names are self-explanatory
    y = NULL
  ) +
  scale_fill_manual(
    values = c(
      "n_pos" = "cadetblue1",      # Blue for positive count
      "n_neg" = "firebrick3"     # Red for negative count
    )
    )+
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    panel.grid.major.x = element_blank(),
    axis.text.x = element_text(size = 11),
    axis.text.y = element_text(size = 11),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    plot.margin = margin(t = 20, r = 20, b = 20, l = 20)
  ) +
  # Format the axis labels to be more readable
  scale_x_discrete(labels = c(
    "n_pos" = "Positive Words", 
    "n_neg" = "Negative Words"
  )) + theme(legend.position = "none")

```

:::
